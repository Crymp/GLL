{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5c755eaab84e9f9e0be9baf49e8f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CytoscapeWidget(cytoscape_layout={'name': 'cola'}, cytoscape_style=[{'selector': 'node', 'css': {'background-câ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import ipycytoscape\n",
    "import networkx as nx\n",
    "\n",
    "def vis(G):\n",
    "    cso = ipycytoscape.CytoscapeWidget()\n",
    "    cso.graph.add_graph_from_networkx(G)\n",
    "    cso.set_style([\n",
    "                            {\n",
    "                                'selector': 'node',\n",
    "                                'css': {\n",
    "                                    'background-color': 'red',\n",
    "                                    'content': 'data(node_label)' #\n",
    "                                }\n",
    "                            },\n",
    "                                                    {\n",
    "                                'selector': 'edge',\n",
    "                                'css': {\n",
    "                                    'content': 'data(edge_label)' #\n",
    "                                }\n",
    "                            }\n",
    "                \n",
    "                ])\n",
    "\n",
    "    for i in range(len(cso.graph.nodes)):\n",
    "        id = int(cso.graph.nodes[i].data['id'])\n",
    "        label = cso.graph.nodes[i].data['node_label']\n",
    "        new_label = f\"{id}: {label}\"\n",
    "        cso.graph.nodes[i].data['node_label'] = new_label\n",
    "\n",
    "\n",
    "    # for i in range(len(cso.graph.edges)):\n",
    "    #     label = cso.graph.edges[i].data['edge_label']\n",
    "    #     new_label = f\"{label}\"\n",
    "    #     cso.graph.edges[i].data['edge_label'] = new_label\n",
    "\n",
    "    return cso\n",
    "    \n",
    "# Test it with output graph\n",
    "import pickle\n",
    "#with open('datasets/DD/data.pkl','rb') as f:\n",
    "with open('../datasets/ZINC_TEST/data.pkl','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "out = vis(data[3])\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SparseGraph:\n",
    "    # Convert a graph to a sparse representation (numpy matrices)\n",
    "    def __init__(self, G, num_Edge_classes=3, num_Node_classes=23):\n",
    "        # Convert a networkx graph (with edge and node labels) to a sparse graph format\n",
    "\n",
    "        # Edge index Matrix\n",
    "        idxs = np.array(G.edges).transpose() # (2,|E|) dim. array idxs[:,j] = [u,v]^T indicates endpoints of j'th edge e=u->v\n",
    "        idxs = np.concatenate((idxs, idxs[[1,0]]), axis=1) # idxs[[1,0]] flips the two rows ie [u,v]^T -> [v,u]^T, so by concat now have (2, 2*|E|)\n",
    "        self.idxs = torch.from_numpy(idxs) #.astype(np.float32))\n",
    "\n",
    "        # Node features\n",
    "        Xv = np.array([G.nodes[idx]['node_label'] for idx in G.nodes]).transpose() # Node feature matrix of dim (reshape: (|V|,) -> (|V|,1))\n",
    "        #Xv = torch.from_numpy(Xv.astype(np.float32))\n",
    "        self.Xv = torch.nn.functional.one_hot(torch.tensor(Xv, dtype=torch.int64), num_classes=23).to(torch.float32)\n",
    "\n",
    "        # Edges features\n",
    "        Xe = np.array([G.edges[idx]['edge_label'] for idx in G.edges]).transpose() # Edge feature matrix of dim (reshape: (|E|,) -> (|E|,1))\n",
    "        Xe = np.concatenate((Xe,Xe), axis=0) - 1 # For some reason class labels are {1,2,3} and not {0,1,2}...\n",
    "        self.Xe = torch.nn.functional.one_hot(torch.tensor(Xe, dtype=torch.int64), num_classes=3).to(torch.float32)\n",
    "\n",
    "        # Get Graph features\n",
    "        y = G.graph['label']\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "\n",
    "        # Set Batch_idx (just here for compability)\n",
    "        self.batch_idx = torch.zeros((Xv.shape[0]), dtype=torch.int64)\n",
    "\n",
    "    def to_gpu(self):\n",
    "        # Transfer all tensors from cpu to gpu/cuda\n",
    "        self.y.to('cuda')\n",
    "        self.idxs.to('cuda')\n",
    "        self.Xe.to('cuda')\n",
    "        self.Xv.to('cuda')\n",
    "        \n",
    "\n",
    "\n",
    "    def to_nx(self):\n",
    "        # TODO Update to account for OHE encoding of vectors\n",
    "        # Convert the sparse graph back to a networkx gaph g\n",
    "\n",
    "        # Convert tensors to numpy\n",
    "        idxs = self.idxs.numpy().astype('int')\n",
    "        Xv = self.Xv.numpy()\n",
    "        Xe = self.Xe.numpy()\n",
    "\n",
    "        g = nx.Graph() # Empty nx graph\n",
    "\n",
    "        # Add edges (nodes added automatically)\n",
    "        for j in range(idxs.shape[1]):\n",
    "            g.add_edge(idxs[0,j], idxs[1,j])\n",
    "        \n",
    "        # Set Node and Edge Weights\n",
    "        nx.set_node_attributes(g, {idx: Xv[idx] for idx in range(Xv.shape[0])}, \"node_label\")\n",
    "        nx.set_edge_attributes(g, {(idxs[0,idx], idxs[1,idx]): Xe[idx] for idx in range(int(Xe.shape[0]/2))}, \"edge_label\")\n",
    "\n",
    "        # TODO: Convert graph label in networkx\n",
    "        return g\n",
    "\n",
    "\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, nx_graph_list):\n",
    "        self.np_sparse_graphs = [SparseGraph(g) for g in nx_graph_list]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.np_sparse_graphs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.np_sparse_graphs[idx]\n",
    "        #return torch.from_numpy(sg.idxs), torch.from_numpy(sg.Xv), torch.from_numpy(sg.Xe), torch.from_numpy(sg.Xe)\n",
    "\n",
    "\n",
    "SG = SparseGraph(data[3])\n",
    "# G1 = SG.to_nx()\n",
    "# vis(G1)\n",
    "\n",
    "def MyCollate(sparse_graph_list):\n",
    "    #sparse_graph_list = [SparseGraph(data[0]), SparseGraph(data[1]), SparseGraph(data[2]) ]\n",
    "    #sgl = sparse_graph_list\n",
    "\n",
    "    # Create empty SparseGraph Object (avoid calling init, we will initialize here alreadt)\n",
    "    output = SparseGraph.__new__(SparseGraph)\n",
    "\n",
    "    # By joining graphs, the node indexes need to she shifted\n",
    "    # Ie if the first graph has 10 nodes, then for the second graph the node indexes 0,1,2,... --> 10,11,12,...\n",
    "\n",
    "    # compute batch_idx matrix, and a lookup table for how much to shift each graph's nodes indexes by\n",
    "    node_idx_shift = [0] # Lookup table for the node index shift of each graph\n",
    "    batch_idx = []\n",
    "    tot_num_nodes = 0 # Total number of nodes\n",
    "    for i,sg in enumerate(sparse_graph_list):\n",
    "        num_nodes = sg.Xv.shape[0]\n",
    "        tot_num_nodes += num_nodes\n",
    "        node_idx_shift.append(tot_num_nodes)\n",
    "        batch_idx += [i]*num_nodes\n",
    "\n",
    "    # First shift all the node indexes in each graph, and concatenate them\n",
    "    output.idxs = torch.cat([sg.idxs + torch.from_numpy(np.array([node_idx_shift[i], node_idx_shift[i]]).transpose().reshape(-1,1))  # idxs + [idx_shift, idx_shift]^T\n",
    "                            for i, sg in enumerate(sparse_graph_list)],\n",
    "                        dim = 1)\n",
    "\n",
    "    # Change batch_idx type to tensor\n",
    "    output.batch_idx = torch.tensor(np.array(batch_idx), dtype=torch.int64)\n",
    "\n",
    "    # Concatenate Node and Edge feature vectors, and graph labels\n",
    "    output.Xv = torch.cat([sg.Xv  for sg in sparse_graph_list])\n",
    "    output.Xe = torch.cat([sg.Xe for sg in sparse_graph_list])\n",
    "    output.y = torch.cat([sg.y for sg in sparse_graph_list])\n",
    "\n",
    "    return output\n",
    "\n",
    "sgl = [SparseGraph(data[0]), SparseGraph(data[1])] #SparseGraph(data[2]) ]\n",
    "res = MyCollate(sgl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch_scatter\n",
    "\n",
    "from torch import nn\n",
    "class GNN_U(torch.nn.Module):\n",
    "    # TODO: Actually implement this! Just dummy so far (!!depth attribute!!)\n",
    "\n",
    "    def __init__(self, in_features, out_features, depth):\n",
    "        super(GNN_U, self).__init__()\n",
    "        self.fc = nn.Linear( in_features, out_features)\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x) \n",
    "        return x\n",
    "\n",
    "class GNN_M(torch.nn.Module):\n",
    "    # TODO: Actually implement this! Just dummy so far (!!depth attribute!!)\n",
    "    def __init__(self, in_features, out_features, depth):\n",
    "        super(GNN_M, self).__init__()\n",
    "        self.fc = nn.Linear( in_features, out_features)\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        self.relu = torch.nn.ReLU() \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        self.dropout(x)\n",
    "        x = self.relu(x) \n",
    "        return x\n",
    "\n",
    "\n",
    "class GNN_layer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, Xe_width, scatter_func='SUM', U_depth=2, M_depth=2, M_width=2):\n",
    "        super(GNN_layer, self).__init__()\n",
    "\n",
    "        # Initialize Scatter function\n",
    "        if type(scatter_func) == type('str'):\n",
    "            if scatter_func.lower()=='sum':\n",
    "                self.scatter_agg = torch_scatter.scatter_sum\n",
    "            elif scatter_func.lower()=='max':\n",
    "                self.scatter_agg = torch_scatter.scatter_max\n",
    "            elif scatter_func.lower()=='mean':\n",
    "                self.scatter_agg = torch_scatter.scatter_mean\n",
    "            else:\n",
    "                import warnings\n",
    "                warnings.warn(\"scatter_function unknown! Defaulting to \\\"SUM\\\"\")\n",
    "                self.scatter_agg = torch_scatter.scatter_add\n",
    "        else: \n",
    "            # Custom scatter function\n",
    "            self.scatter_agg = scatter_func\n",
    "\n",
    "        # Initialize M and U Neural Nets\n",
    "        self.M = GNN_M(in_features + Xe_width, M_width, M_depth)\n",
    "        self.U = GNN_U(in_features + M_width, out_features, U_depth)\n",
    "\n",
    "        # Define parameter list (needed for optimizer)\n",
    "        self.param_list = list(self.M.parameters()) + list( self.U.parameters())\n",
    "\n",
    "    def forward(self, H, sparse_graph):\n",
    "        Y = self.M.forward(torch.cat((H[sparse_graph.idxs[0,:]], sparse_graph.Xe), dim=1)) # (2|E|, in_features + Xe_width) -> (2|E|, M_width)\n",
    "        # TODO: Special case for max\n",
    "        Z = self.scatter_agg(Y, sparse_graph.idxs[1,:], dim=0) # (2|E|, M_width) -> (|V|, M_width)\n",
    "        return self.U.forward(torch.cat((H,Z), dim=1)) # (|V|, H_width + M_width) -> (|V|, out_features)\n",
    "\n",
    "class GNN_skip_layer(GNN_layer):\n",
    "    # Wraps a GNN_layer with a skip connection (note out_features=in_features enforced, otherwise identical)\n",
    "\n",
    "    def __init__(self, in_features, Xe_width, scatter_func='SUM', U_depth=2, M_depth=2, M_width=2):\n",
    "        # Identical to GNN_layer, just that now out_features=in_features\n",
    "        super(GNN_skip_layer, self).__init__(in_features, in_features, Xe_width, scatter_func='SUM', U_depth=2, M_depth=2, M_width=2)\n",
    "\n",
    "    def forward(self, H, sparse_graph):\n",
    "        return H + super(GNN_skip_layer, self).forward(H, sparse_graph)\n",
    "\n",
    "\n",
    "class GNN_pool(torch.nn.Module):\n",
    "    def __init__(self, scatter_func='sum'):\n",
    "        super(GNN_pool, self).__init__()\n",
    "        \n",
    "        # Initialize Scatter function\n",
    "        if type(scatter_func) == type('str'):\n",
    "            if scatter_func.lower()=='sum':\n",
    "                self.scatter_agg = torch_scatter.scatter_sum\n",
    "            elif scatter_func.lower()=='max':\n",
    "                self.scatter_agg = torch_scatter.scatter_max\n",
    "            elif scatter_func.lower()=='mean':\n",
    "                self.scatter_agg = torch_scatter.scatter_mean\n",
    "            else:\n",
    "                import warnings\n",
    "                warnings.warn(\"scatter_function unknown! Defaulting to \\\"SUM\\\"\")\n",
    "                self.scatter_agg = torch_scatter.scatter_add\n",
    "        else: \n",
    "            # Custom scatter function\n",
    "            self.scatter_agg = scatter_func\n",
    "\n",
    "        # Parameter list (empty, just here for compatabillity)\n",
    "        self.param_list = []\n",
    "\n",
    "    def forward(self, H, sparse_graph):\n",
    "        return torch_scatter.scatter_sum(H, sparse_graph.batch_idx, dim=0)\n",
    "\n",
    "\n",
    "class GNN_virtual_node(torch.nn.Module):\n",
    "    # Virtual Node\n",
    "\n",
    "    def __init__(self, in_features):\n",
    "        super(GNN_virtual_node, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, in_features)\n",
    "        self.relu = torch.nn.ReLU() \n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.param_list = self.parameters()\n",
    "\n",
    "    def forward(self, H, sparse_graph):\n",
    "        # Compute sum over all nodes for each graph, H_sum_graph[i] = sum of H over all nodes in the i'th subgrpah\n",
    "        H_sum_graph = torch_scatter.scatter_sum(H, sparse_graph.batch_idx, dim=0)\n",
    "        H_sum_graph = self.fc(H_sum_graph)\n",
    "        H_sum_graph = self.dropout(H_sum_graph)\n",
    "        H_sum_graph = self.relu(H_sum_graph)\n",
    "\n",
    "        # Cast graph sum back to every node, and then add to H (skip connection)\n",
    "        return H + H_sum_graph[sparse_graph.batch_idx]\n",
    "\n",
    "\n",
    "class GNN_wrapper():\n",
    "    # Wraps a module like nn.Linear which only applied to H (and not (H, sparse_graph)\n",
    "    def __init__(self, module):\n",
    "        self.module = module\n",
    "        self.param_list = self.module.parameters() \n",
    "\n",
    "    def forward(self, H, sparse_graph):\n",
    "        return self.module.forward(H)\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, Xv_width, Xe_width):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.layers = []\n",
    "        \n",
    "        # First layer, (input dimension must match the initial dimension)\n",
    "        self.layers.append(GNN_layer(in_features=Xv_width, out_features=10, Xe_width=Xe_width, scatter_func='mean', U_depth=2, M_depth=2, M_width=2))\n",
    "\n",
    "        self.layers.append(GNN_virtual_node(10))\n",
    "        # TODO: Add more layers (and actually figure out what needs to be done here)\n",
    "\n",
    "        self.layers.append(GNN_skip_layer(in_features=10, Xe_width=Xe_width, scatter_func='mean', U_depth=2, M_depth=2, M_width=2))\n",
    "        self.layers.append(GNN_virtual_node(10))\n",
    "        \n",
    "        # self.layers.append(GNN_skip_layer(in_features=10, Xe_width=Xe_width, scatter_func='mean', U_depth=2, M_depth=2, M_width=2))\n",
    "        # self.layers.append(GNN_virtual_node(10))\n",
    "        \n",
    "        # self.layers.append(GNN_skip_layer(in_features=10, Xe_width=Xe_width, scatter_func='mean', U_depth=2, M_depth=2, M_width=2))\n",
    "        # self.layers.append(GNN_virtual_node(10))\n",
    "        \n",
    "        # Final layer (For node level classication, should be exactly the size of the node output, for anything else, it should be different)\n",
    "        self.layers.append(GNN_layer(in_features=10, out_features=10, Xe_width=Xe_width, scatter_func='mean', U_depth=2, M_depth=2, M_width=2))\n",
    "        self.layers.append(GNN_wrapper(torch.nn.Linear(10,1)))\n",
    "        self.layers.append(GNN_pool('mean'))\n",
    "\n",
    "        # Build list of parameters (needed for optimizer)\n",
    "        self.param_list = []\n",
    "        for layer in self.layers:\n",
    "            self.param_list += layer.param_list\n",
    "\n",
    "\n",
    "    def forward(self, sparse_graph):\n",
    "        # Initial Hidden node layers\n",
    "        H = sparse_graph.Xv\n",
    "\n",
    "        # Reshape if neccessary\n",
    "        if len(H.shape)==1:\n",
    "            H = H.reshape((-1,1))\n",
    "\n",
    "        # Reshape Edge feature matrix if neccessarry\n",
    "        if len(sparse_graph.Xe.shape)==1:\n",
    "            sparse_graph.Xe = sparse_graph.Xe.reshape((-1,1))\n",
    "            import warnings\n",
    "            warnings.warn(\"Needed to reshape Xe!!\")\n",
    "\n",
    "        # Actual forward Pass of H through layers\n",
    "        for layer in self.layers:\n",
    "            H = layer.forward(H, sparse_graph)\n",
    "        return H        \n",
    "\n",
    "#Simple Example\n",
    "gnn = GNN(Xv_width=23, Xe_width=3)\n",
    "#sparse_graph = ZINC2sparse(data[0])\n",
    "sparse_graph = MyCollate([SparseGraph(data[0]), SparseGraph(data[1]), SparseGraph(data[2])])\n",
    "H = gnn.forward(sparse_graph)\n",
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datasets and initliaze dataloaders\n",
    "import pickle\n",
    "with open('../datasets/ZINC_Train/data.pkl','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    train_loader = torch.utils.data.DataLoader(MyDataset(data), batch_size=15, collate_fn=MyCollate)\n",
    "\n",
    "with open('../datasets/ZINC_Test/data.pkl','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    test_loader = torch.utils.data.DataLoader(MyDataset(data), batch_size=15, collate_fn=MyCollate)\n",
    "\n",
    "with open('../datasets/ZINC_Val/data.pkl','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    validate_loader = torch.utils.data.DataLoader(MyDataset(data), batch_size=15, collate_fn=MyCollate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Train_loss = 31.878885597303352, Validation_loss = 9.103466055286464\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(gnn_model, dataloader, optimizer, loss_fn):\n",
    "    # Trains a gnn model for one epoch\n",
    "    gnn_model.train()\n",
    "    sum_loss = 0\n",
    "    counter = 0\n",
    "    for sparse_graph in dataloader:\n",
    "        if use_gpu:\n",
    "            sparse_graph.to_gpu()\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # do forward pass\n",
    "        output = gnn_model.forward(sparse_graph).reshape((-1))\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = loss_fn(output, sparse_graph.y)\n",
    "        sum_loss += loss.item()\n",
    "        counter += 1\n",
    "        \n",
    "        # backpropagate loss and do parameter updates\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return gnn_model, sum_loss / counter\n",
    "\n",
    "def validate(gnn_model, dataloader, loss_fn):\n",
    "    # Trains a gnn model for one epoch\n",
    "    gnn_model.eval()\n",
    "    \n",
    "    sum_loss = 0\n",
    "    counter = 0\n",
    "    for sparse_graph in dataloader:\n",
    "        output = gnn_model.forward(sparse_graph).reshape((-1))\n",
    "        loss = loss_fn(output, sparse_graph.y)\n",
    "        sum_loss += loss.item()\n",
    "        counter += 1\n",
    "    return sum_loss / counter\n",
    "\n",
    "import torch.optim as optim\n",
    "gnn = GNN(Xv_width=23, Xe_width=3)\n",
    "optimizer = optim.Adam(gnn.param_list, lr=0.0001)\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "gnn.train()\n",
    "\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    gnn.to('cuda')\n",
    "\n",
    "for epoch in range(50):\n",
    "    gnn, train_loss = train_epoch(gnn, train_loader, optimizer, loss_fn)\n",
    "    val_loss = validate(gnn, validate_loader, loss_fn)\n",
    "    print(f\"{epoch}: Train_loss = {train_loss}, Validation_loss = {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1050 Ti with Max-Q Design'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('gll')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3325763147d7b6a719f3ec3969238f3ee9907e3239cd74fe71b5fc36bf844357"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
