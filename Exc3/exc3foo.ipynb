{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a12a55a11d745c8a6ce81d3133d21c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CytoscapeWidget(cytoscape_layout={'name': 'cola'}, cytoscape_style=[{'selector': 'node', 'css': {'background-câ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import ipycytoscape\n",
    "import networkx as nx\n",
    "\n",
    "def vis(G):\n",
    "    cso = ipycytoscape.CytoscapeWidget()\n",
    "    cso.graph.add_graph_from_networkx(G)\n",
    "    cso.set_style([\n",
    "                            {\n",
    "                                'selector': 'node',\n",
    "                                'css': {\n",
    "                                    'background-color': 'red',\n",
    "                                    'content': 'data(node_label)' #\n",
    "                                }\n",
    "                            },\n",
    "                                                    {\n",
    "                                'selector': 'edge',\n",
    "                                'css': {\n",
    "                                    'content': 'data(edge_label)' #\n",
    "                                }\n",
    "                            }\n",
    "                \n",
    "                ])\n",
    "\n",
    "    for i in range(len(cso.graph.nodes)):\n",
    "        id = int(cso.graph.nodes[i].data['id'])\n",
    "        label = cso.graph.nodes[i].data['node_label']\n",
    "        new_label = f\"{id}: {label}\"\n",
    "        cso.graph.nodes[i].data['node_label'] = new_label\n",
    "\n",
    "\n",
    "    # for i in range(len(cso.graph.edges)):\n",
    "    #     label = cso.graph.edges[i].data['edge_label']\n",
    "    #     new_label = f\"{label}\"\n",
    "    #     cso.graph.edges[i].data['edge_label'] = new_label\n",
    "\n",
    "    return cso\n",
    "    \n",
    "# Test it with output graph\n",
    "import pickle\n",
    "#with open('datasets/DD/data.pkl','rb') as f:\n",
    "with open('../datasets/ZINC_TEST/data.pkl','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "out = vis(data[3])\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SparseGraph:\n",
    "    # Convert a graph to a sparse representation (numpy matrices)\n",
    "    def __init__(self, G, num_Edge_classes=3, num_Node_classes=23):\n",
    "        # Convert a networkx graph (with edge and node labels) to a sparse graph format\n",
    "\n",
    "        # Edge index Matrix\n",
    "        idxs = np.array(G.edges).transpose() # (2,|E|) dim. array idxs[:,j] = [u,v]^T indicates endpoints of j'th edge e=u->v\n",
    "        idxs = np.concatenate((idxs, idxs[[1,0]]), axis=1) # idxs[[1,0]] flips the two rows ie [u,v]^T -> [v,u]^T, so by concat now have (2, 2*|E|)\n",
    "        self.idxs = torch.from_numpy(idxs) #.astype(np.float32))\n",
    "\n",
    "        # Node features\n",
    "        Xv = np.array([G.nodes[idx]['node_label'] for idx in G.nodes]).transpose() # Node feature matrix of dim (reshape: (|V|,) -> (|V|,1))\n",
    "        #Xv = torch.from_numpy(Xv.astype(np.float32))\n",
    "        self.Xv = torch.nn.functional.one_hot(torch.tensor(Xv, dtype=torch.int64), num_classes=23).to(torch.float32)\n",
    "\n",
    "        # Edges features\n",
    "        Xe = np.array([G.edges[idx]['edge_label'] for idx in G.edges]).transpose() # Edge feature matrix of dim (reshape: (|E|,) -> (|E|,1))\n",
    "        Xe = np.concatenate((Xe,Xe), axis=0) - 1 # For some reason class labels are {1,2,3} and not {0,1,2}...\n",
    "        self.Xe = torch.nn.functional.one_hot(torch.tensor(Xe, dtype=torch.int64), num_classes=3).to(torch.float32)\n",
    "\n",
    "        # Get Graph features\n",
    "        y = G.graph['label']\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "\n",
    "        # Set Batch_idx (just here for compability)\n",
    "        self.batch_idx = torch.zeros((Xv.shape[0]), dtype=torch.int64)\n",
    "\n",
    "    def to_gpu(self):\n",
    "        # Transfer all tensors from cpu to gpu/cuda\n",
    "        self.y.to('cuda')\n",
    "        self.idxs.to('cuda')\n",
    "        self.Xe.to('cuda')\n",
    "        self.Xv.to('cuda')\n",
    "        \n",
    "\n",
    "\n",
    "    def to_nx(self):\n",
    "        # TODO Update to account for OHE encoding of vectors\n",
    "        # Convert the sparse graph back to a networkx gaph g\n",
    "\n",
    "        # Convert tensors to numpy\n",
    "        idxs = self.idxs.numpy().astype('int')\n",
    "        Xv = self.Xv.numpy()\n",
    "        Xe = self.Xe.numpy()\n",
    "\n",
    "        g = nx.Graph() # Empty nx graph\n",
    "\n",
    "        # Add edges (nodes added automatically)\n",
    "        for j in range(idxs.shape[1]):\n",
    "            g.add_edge(idxs[0,j], idxs[1,j])\n",
    "        \n",
    "        # Set Node and Edge Weights\n",
    "        nx.set_node_attributes(g, {idx: Xv[idx] for idx in range(Xv.shape[0])}, \"node_label\")\n",
    "        nx.set_edge_attributes(g, {(idxs[0,idx], idxs[1,idx]): Xe[idx] for idx in range(int(Xe.shape[0]/2))}, \"edge_label\")\n",
    "\n",
    "        # TODO: Convert graph label in networkx\n",
    "        return g\n",
    "\n",
    "\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, nx_graph_list):\n",
    "        self.np_sparse_graphs = [SparseGraph(g) for g in nx_graph_list]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.np_sparse_graphs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.np_sparse_graphs[idx]\n",
    "        #return torch.from_numpy(sg.idxs), torch.from_numpy(sg.Xv), torch.from_numpy(sg.Xe), torch.from_numpy(sg.Xe)\n",
    "\n",
    "\n",
    "SG = SparseGraph(data[3])\n",
    "# G1 = SG.to_nx()\n",
    "# vis(G1)\n",
    "\n",
    "def MyCollate(sparse_graph_list):\n",
    "    #sparse_graph_list = [SparseGraph(data[0]), SparseGraph(data[1]), SparseGraph(data[2]) ]\n",
    "    #sgl = sparse_graph_list\n",
    "\n",
    "    # Create empty SparseGraph Object (avoid calling init, we will initialize here alreadt)\n",
    "    output = SparseGraph.__new__(SparseGraph)\n",
    "\n",
    "    # By joining graphs, the node indexes need to she shifted\n",
    "    # Ie if the first graph has 10 nodes, then for the second graph the node indexes 0,1,2,... --> 10,11,12,...\n",
    "\n",
    "    # compute batch_idx matrix, and a lookup table for how much to shift each graph's nodes indexes by\n",
    "    node_idx_shift = [0] # Lookup table for the node index shift of each graph\n",
    "    batch_idx = []\n",
    "    tot_num_nodes = 0 # Total number of nodes\n",
    "    for i,sg in enumerate(sparse_graph_list):\n",
    "        num_nodes = sg.Xv.shape[0]\n",
    "        tot_num_nodes += num_nodes\n",
    "        node_idx_shift.append(tot_num_nodes)\n",
    "        batch_idx += [i]*num_nodes\n",
    "\n",
    "    # First shift all the node indexes in each graph, and concatenate them\n",
    "    output.idxs = torch.cat([sg.idxs + torch.from_numpy(np.array([node_idx_shift[i], node_idx_shift[i]]).transpose().reshape(-1,1))  # idxs + [idx_shift, idx_shift]^T\n",
    "                            for i, sg in enumerate(sparse_graph_list)],\n",
    "                        dim = 1)\n",
    "\n",
    "    # Change batch_idx type to tensor\n",
    "    output.batch_idx = torch.tensor(np.array(batch_idx), dtype=torch.int64)\n",
    "\n",
    "    # Concatenate Node and Edge feature vectors, and graph labels\n",
    "    output.Xv = torch.cat([sg.Xv  for sg in sparse_graph_list])\n",
    "    output.Xe = torch.cat([sg.Xe for sg in sparse_graph_list])\n",
    "    output.y = torch.cat([sg.y for sg in sparse_graph_list])\n",
    "\n",
    "    return output\n",
    "\n",
    "sgl = [SparseGraph(data[0]), SparseGraph(data[1])] #SparseGraph(data[2]) ]\n",
    "res = MyCollate(sgl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch_scatter\n",
    "\n",
    "from torch import nn\n",
    "class GNN_U(torch.nn.Module):\n",
    "    # TODO: Actually implement this! Just dummy so far (!!depth attribute!!)\n",
    "\n",
    "    def __init__(self, in_features, out_features, depth):\n",
    "        super(GNN_U, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        self.bn = torch.nn.BatchNorm1d(num_features=in_features)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x) \n",
    "        return x\n",
    "\n",
    "class GNN_M(torch.nn.Module):\n",
    "    # TODO: Actually implement this! Just dummy so far (!!depth attribute!!)\n",
    "    def __init__(self, in_features, out_features, depth):\n",
    "        super(GNN_M, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        self.bn = torch.nn.BatchNorm1d(num_features=in_features)\n",
    "        self.relu = torch.nn.ReLU() \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = self.fc(x)\n",
    "        self.dropout(x)\n",
    "        x = self.relu(x) \n",
    "        return x\n",
    "\n",
    "\n",
    "class GNN_layer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, Xe_width, scatter_func='SUM', U_depth=2, M_depth=2, M_width=2):\n",
    "        super(GNN_layer, self).__init__()\n",
    "\n",
    "        # Initialize Scatter function\n",
    "        if type(scatter_func) == type('str'):\n",
    "            if scatter_func.lower()=='sum':\n",
    "                self.scatter_agg = torch_scatter.scatter_sum\n",
    "            elif scatter_func.lower()=='max':\n",
    "                self.scatter_agg = torch_scatter.scatter_max\n",
    "            elif scatter_func.lower()=='mean':\n",
    "                self.scatter_agg = torch_scatter.scatter_mean\n",
    "            else:\n",
    "                import warnings\n",
    "                warnings.warn(\"scatter_function unknown! Defaulting to \\\"SUM\\\"\")\n",
    "                self.scatter_agg = torch_scatter.scatter_add\n",
    "        else: \n",
    "            # Custom scatter function\n",
    "            self.scatter_agg = scatter_func\n",
    "\n",
    "        # Initialize M and U Neural Nets\n",
    "        self.M = GNN_M(in_features + Xe_width, M_width, M_depth)\n",
    "        self.U = GNN_U(in_features + M_width, out_features, U_depth)\n",
    "\n",
    "        # Define parameter list (needed for optimizer)\n",
    "        self.param_list = list(self.M.parameters()) + list( self.U.parameters())\n",
    "\n",
    "    def forward(self, H, sparse_graph):\n",
    "        Y = self.M.forward(torch.cat((H[sparse_graph.idxs[0,:]], sparse_graph.Xe), dim=1)) # (2|E|, in_features + Xe_width) -> (2|E|, M_width)\n",
    "        # TODO: Special case for max\n",
    "        Z = self.scatter_agg(Y, sparse_graph.idxs[1,:], dim=0) # (2|E|, M_width) -> (|V|, M_width)\n",
    "        return self.U.forward(torch.cat((H,Z), dim=1)) # (|V|, H_width + M_width) -> (|V|, out_features)\n",
    "\n",
    "class GNN_skip_layer(GNN_layer):\n",
    "    # Wraps a GNN_layer with a skip connection (note out_features=in_features enforced, otherwise identical)\n",
    "\n",
    "    def __init__(self, in_features, Xe_width, scatter_func='SUM', U_depth=2, M_depth=2, M_width=2):\n",
    "        # Identical to GNN_layer, just that now out_features=in_features\n",
    "        super(GNN_skip_layer, self).__init__(in_features, in_features, Xe_width, scatter_func='SUM', U_depth=2, M_depth=2, M_width=2)\n",
    "\n",
    "    def forward(self, H, sparse_graph):\n",
    "        return H + super(GNN_skip_layer, self).forward(H, sparse_graph)\n",
    "\n",
    "\n",
    "class GNN_pool(torch.nn.Module):\n",
    "    def __init__(self, scatter_func='sum'):\n",
    "        super(GNN_pool, self).__init__()\n",
    "        \n",
    "        # Initialize Scatter function\n",
    "        if type(scatter_func) == type('str'):\n",
    "            if scatter_func.lower()=='sum':\n",
    "                self.scatter_agg = torch_scatter.scatter_sum\n",
    "            elif scatter_func.lower()=='max':\n",
    "                self.scatter_agg = torch_scatter.scatter_max\n",
    "            elif scatter_func.lower()=='mean':\n",
    "                self.scatter_agg = torch_scatter.scatter_mean\n",
    "            else:\n",
    "                import warnings\n",
    "                warnings.warn(\"scatter_function unknown! Defaulting to \\\"SUM\\\"\")\n",
    "                self.scatter_agg = torch_scatter.scatter_add\n",
    "        else: \n",
    "            # Custom scatter function\n",
    "            self.scatter_agg = scatter_func\n",
    "\n",
    "        # Parameter list (empty, just here for compatabillity)\n",
    "        self.param_list = []\n",
    "\n",
    "    def forward(self, H, sparse_graph):\n",
    "        return torch_scatter.scatter_sum(H, sparse_graph.batch_idx, dim=0)\n",
    "\n",
    "\n",
    "class GNN_virtual_node(torch.nn.Module):\n",
    "    # Virtual Node\n",
    "\n",
    "    def __init__(self, in_features):\n",
    "        super(GNN_virtual_node, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, in_features)\n",
    "        self.relu = torch.nn.ReLU() \n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.param_list = self.parameters()\n",
    "\n",
    "    def forward(self, H, sparse_graph):\n",
    "        # Compute sum over all nodes for each graph, H_sum_graph[i] = sum of H over all nodes in the i'th subgaph\n",
    "        H_sum_graph = torch_scatter.scatter_sum(H, sparse_graph.batch_idx, dim=0)\n",
    "        H_sum_graph = self.fc(H_sum_graph)\n",
    "        H_sum_graph = self.dropout(H_sum_graph)\n",
    "        H_sum_graph = self.relu(H_sum_graph)\n",
    "\n",
    "        # Cast graph sum back to every node, and then add to H (skip connection)\n",
    "        return H + H_sum_graph[sparse_graph.batch_idx]\n",
    "\n",
    "\n",
    "class GNN_wrapper():\n",
    "    # Wraps a module like nn.Linear which only applied to H (and not (H, sparse_graph)\n",
    "    def __init__(self, module):\n",
    "        self.module = module\n",
    "        self.param_list = self.module.parameters() \n",
    "\n",
    "    def forward(self, H, sparse_graph):\n",
    "        return self.module.forward(H)\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, Xv_width=23, Xe_width=3):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.layers = []\n",
    "        \n",
    "        # First layer, (input dimension must match the initial dimension)\n",
    "        self.layers.append(GNN_layer(in_features=Xv_width, out_features=10, Xe_width=Xe_width, scatter_func='mean', U_depth=2, M_depth=2, M_width=5))\n",
    "\n",
    "        self.layers.append(GNN_virtual_node(10))\n",
    "        # TODO: Add more layers (and actually figure out what needs to be done here)\n",
    "\n",
    "        self.layers.append(GNN_wrapper(torch.nn.BatchNorm1d(num_features=10)))\n",
    "\n",
    "        self.layers.append(GNN_skip_layer(in_features=10, Xe_width=Xe_width, scatter_func='mean', U_depth=2, M_depth=4, M_width=5))\n",
    "        self.layers.append(GNN_virtual_node(10)) \n",
    "\n",
    "        self.layers.append(GNN_wrapper(torch.nn.BatchNorm1d(num_features=10)))\n",
    "        \n",
    "        self.layers.append(GNN_skip_layer(in_features=10, Xe_width=Xe_width, scatter_func='mean', U_depth=2, M_depth=4, M_width=5))\n",
    "        self.layers.append(GNN_virtual_node(10))\n",
    "        \n",
    "        self.layers.append(GNN_wrapper(torch.nn.BatchNorm1d(num_features=10)))\n",
    "\n",
    "        # Final layer (For node level classication, should be exactly the size of the node output, for anything else, it should be different)\n",
    "        self.layers.append(GNN_skip_layer(in_features=10, Xe_width=Xe_width, scatter_func='mean', U_depth=2, M_depth=4, M_width=5))\n",
    "        \n",
    "        self.layers.append(GNN_wrapper(torch.nn.Linear(10,1)))\n",
    "        self.layers.append(GNN_pool('mean'))\n",
    "\n",
    "        # Build list of parameters (needed for optimizer)\n",
    "        self.param_list = []\n",
    "        for layer in self.layers:\n",
    "            self.param_list += layer.param_list\n",
    "\n",
    "    def forward(self, sparse_graph):\n",
    "        # Initial Hidden node layers\n",
    "        H = sparse_graph.Xv\n",
    "\n",
    "        # Reshape if neccessary\n",
    "        if len(H.shape)==1:\n",
    "            H = H.reshape((-1,1))\n",
    "\n",
    "        # Reshape Edge feature matrix if neccessarry\n",
    "        if len(sparse_graph.Xe.shape)==1:\n",
    "            sparse_graph.Xe = sparse_graph.Xe.reshape((-1,1))\n",
    "            import warnings\n",
    "            warnings.warn(\"Needed to reshape Xe!!\")\n",
    "\n",
    "        # Actual forward Pass of H through layers\n",
    "        for layer in self.layers:\n",
    "            H = layer.forward(H, sparse_graph)\n",
    "        return H        \n",
    "\n",
    "#Simple Example\n",
    "gnn = GNN(Xv_width=23, Xe_width=3)\n",
    "#sparse_graph = ZINC2sparse(data[0])\n",
    "sparse_graph = MyCollate([SparseGraph(data[0]), SparseGraph(data[1]), SparseGraph(data[2])])\n",
    "H = gnn.forward(sparse_graph)\n",
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datasets and initliaze dataloaders\n",
    "import pickle\n",
    "with open('../datasets/ZINC_Train/data.pkl','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    train_loader = torch.utils.data.DataLoader(MyDataset(data), batch_size=15, collate_fn=MyCollate)\n",
    "\n",
    "with open('../datasets/ZINC_Test/data.pkl','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    test_loader = torch.utils.data.DataLoader(MyDataset(data), batch_size=15, collate_fn=MyCollate)\n",
    "\n",
    "with open('../datasets/ZINC_Val/data.pkl','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    validate_loader = torch.utils.data.DataLoader(MyDataset(data), batch_size=15, collate_fn=MyCollate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Train_loss = 2.3827184873006155, Validation_loss = 1.3118756618072738\n",
      "1: Train_loss = 1.2429236155757304, Validation_loss = 1.1805253153416648\n",
      "2: Train_loss = 1.141206757641625, Validation_loss = 1.08004003852161\n",
      "3: Train_loss = 1.0871301933207076, Validation_loss = 1.0267877881206684\n",
      "4: Train_loss = 1.0320437564842704, Validation_loss = 0.9869269639698427\n",
      "5: Train_loss = 0.9957367067394228, Validation_loss = 0.9403722619832452\n",
      "6: Train_loss = 0.9737882800009298, Validation_loss = 0.9423933536259096\n",
      "7: Train_loss = 0.9436676924464584, Validation_loss = 0.9154799415994046\n",
      "8: Train_loss = 0.9239645704425972, Validation_loss = 0.8895247970054398\n",
      "9: Train_loss = 0.8988499244203095, Validation_loss = 0.8796977979033741\n",
      "10: Train_loss = 0.8822453247642946, Validation_loss = 0.8455070110399332\n",
      "11: Train_loss = 0.8645413647080469, Validation_loss = 0.829442030458308\n",
      "12: Train_loss = 0.8431167578411245, Validation_loss = 0.8128931055318064\n",
      "13: Train_loss = 0.8265766232446216, Validation_loss = 0.7997210083612755\n",
      "14: Train_loss = 0.8192642267229318, Validation_loss = 0.8041045238722616\n",
      "15: Train_loss = 0.8017895663159899, Validation_loss = 0.7874638958653407\n",
      "16: Train_loss = 0.783342515689501, Validation_loss = 0.799406268258593\n",
      "17: Train_loss = 0.7742844866044161, Validation_loss = 0.7679173675046038\n",
      "18: Train_loss = 0.7569995579467542, Validation_loss = 0.7757674015280026\n",
      "19: Train_loss = 0.7508958032999796, Validation_loss = 0.7583291784151277\n",
      "20: Train_loss = 0.7377123628509813, Validation_loss = 0.7275849171538851\n",
      "21: Train_loss = 0.7219704801532282, Validation_loss = 0.7200620063205263\n",
      "22: Train_loss = 0.7205131532012791, Validation_loss = 0.7321577614812709\n",
      "23: Train_loss = 0.713466221469453, Validation_loss = 0.7407894694983069\n",
      "24: Train_loss = 0.712725286049464, Validation_loss = 0.7135487888286363\n",
      "25: Train_loss = 0.7044136615573376, Validation_loss = 0.7244391009878757\n",
      "26: Train_loss = 0.6961384252987165, Validation_loss = 0.6962630637546083\n",
      "27: Train_loss = 0.692693170623622, Validation_loss = 0.6989692129305939\n",
      "28: Train_loss = 0.6905966751847131, Validation_loss = 0.7068548825249743\n",
      "29: Train_loss = 0.6860637213068566, Validation_loss = 0.6918665330801437\n",
      "30: Train_loss = 0.6837424964070141, Validation_loss = 0.69045120255271\n",
      "31: Train_loss = 0.6764846197266629, Validation_loss = 0.693121034707596\n",
      "32: Train_loss = 0.6656307719949482, Validation_loss = 0.6731264982650529\n",
      "33: Train_loss = 0.6634676500238936, Validation_loss = 0.6788865136566447\n",
      "34: Train_loss = 0.6597845331392903, Validation_loss = 0.6776415745713817\n",
      "35: Train_loss = 0.6611885479811964, Validation_loss = 0.6550560864939619\n",
      "36: Train_loss = 0.653278609645599, Validation_loss = 0.6572612483999623\n",
      "37: Train_loss = 0.6575453250796005, Validation_loss = 0.6592559632080705\n",
      "38: Train_loss = 0.6470050794788744, Validation_loss = 0.6550939728074999\n",
      "39: Train_loss = 0.6489021437859785, Validation_loss = 0.6543212435138759\n",
      "40: Train_loss = 0.64162002155359, Validation_loss = 0.6670060451350995\n",
      "41: Train_loss = 0.6394763880002982, Validation_loss = 0.644954059995822\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [52], line 52\u001b[0m\n\u001b[0;32m     47\u001b[0m     gnn\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m250\u001b[39m):\n\u001b[1;32m---> 52\u001b[0m     gnn, train_loss \u001b[39m=\u001b[39m train_epoch(gnn, train_loader, optimizer, loss_fn)\n\u001b[0;32m     53\u001b[0m     val_loss \u001b[39m=\u001b[39m validate(gnn, validate_loader, loss_fn)\n\u001b[0;32m     54\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m: Train_loss = \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m, Validation_loss = \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [52], line 23\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(gnn_model, dataloader, optimizer, loss_fn)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[39m# backpropagate loss and do parameter updates\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 23\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     24\u001b[0m \u001b[39mreturn\u001b[39;00m gnn_model, sum_loss \u001b[39m/\u001b[39m counter\n",
      "File \u001b[1;32mc:\\Users\\holtm\\anaconda3\\envs\\gll\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\holtm\\anaconda3\\envs\\gll\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\holtm\\anaconda3\\envs\\gll\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    235\u001b[0m          grads,\n\u001b[0;32m    236\u001b[0m          exp_avgs,\n\u001b[0;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    239\u001b[0m          state_steps,\n\u001b[0;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[0;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\holtm\\anaconda3\\envs\\gll\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m func(params,\n\u001b[0;32m    301\u001b[0m      grads,\n\u001b[0;32m    302\u001b[0m      exp_avgs,\n\u001b[0;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    305\u001b[0m      state_steps,\n\u001b[0;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\holtm\\anaconda3\\envs\\gll\\lib\\site-packages\\torch\\optim\\adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    408\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 410\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[0;32m    412\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_epoch(gnn_model, dataloader, optimizer, loss_fn):\n",
    "    # Trains a gnn model for one epoch\n",
    "    gnn_model.train()\n",
    "    sum_loss = 0\n",
    "    counter = 0\n",
    "    for sparse_graph in dataloader:\n",
    "        if use_gpu:\n",
    "            sparse_graph.to_gpu()\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # do forward pass\n",
    "        output = gnn_model.forward(sparse_graph).reshape((-1))\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = loss_fn(output, sparse_graph.y)\n",
    "        sum_loss += loss.item()\n",
    "        counter += 1\n",
    "        \n",
    "        # backpropagate loss and do parameter updates\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return gnn_model, sum_loss / counter\n",
    "\n",
    "def validate(gnn_model, dataloader, loss_fn):\n",
    "    # Trains a gnn model for one epoch\n",
    "    gnn_model.eval()\n",
    "\n",
    "    if use_gpu:\n",
    "        sparse_graph.to_gpu()\n",
    "\n",
    "    \n",
    "    sum_loss = 0\n",
    "    counter = 0\n",
    "    for sparse_graph in dataloader:\n",
    "        output = gnn_model.forward(sparse_graph).reshape((-1))\n",
    "        loss = loss_fn(output, sparse_graph.y)\n",
    "        sum_loss += loss.item()\n",
    "        counter += 1\n",
    "    return sum_loss / counter\n",
    "\n",
    "import torch.optim as optim\n",
    "gnn = GNN(Xv_width=23, Xe_width=3)\n",
    "optimizer = optim.Adam(gnn.param_list, lr=0.001)\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "gnn.train()\n",
    "\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    gnn.to('cuda')\n",
    "\n",
    "for epoch in range(250):\n",
    "\n",
    "\n",
    "    gnn, train_loss = train_epoch(gnn, train_loader, optimizer, loss_fn)\n",
    "    val_loss = validate(gnn, validate_loader, loss_fn)\n",
    "    print(f\"{epoch}: Train_loss = {train_loss}, Validation_loss = {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\holtm\\anaconda3\\envs\\gll\\lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\holtm\\anaconda3\\envs\\gll\\lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "mean = 0\n",
    "counter = 0\n",
    "for g in data:\n",
    "    mean += SparseGraph(g).y\n",
    "mean = mean / counter\n",
    "\n",
    "\n",
    "sum_loss = 0\n",
    "counter = 0\n",
    "for sparse_graph in validate_loader:\n",
    "    loss = loss_fn(mean, sparse_graph.y)\n",
    "    sum_loss += loss.item()\n",
    "    counter += 1\n",
    "mean_loss = sum_loss / counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7130668834963841"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(gnn, test_loader, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('gll')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3325763147d7b6a719f3ec3969238f3ee9907e3239cd74fe71b5fc36bf844357"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
