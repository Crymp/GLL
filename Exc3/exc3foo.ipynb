{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489b13cab45341f09ccd613e1d462378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CytoscapeWidget(cytoscape_layout={'name': 'cola'}, cytoscape_style=[{'selector': 'node', 'css': {'background-câ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import ipycytoscape\n",
    "import networkx as nx\n",
    "\n",
    "def vis(G):\n",
    "    cso = ipycytoscape.CytoscapeWidget()\n",
    "    cso.graph.add_graph_from_networkx(G)\n",
    "    cso.set_style([\n",
    "                            {\n",
    "                                'selector': 'node',\n",
    "                                'css': {\n",
    "                                    'background-color': 'red',\n",
    "                                    'content': 'data(node_label)' #\n",
    "                                }\n",
    "                            },\n",
    "                                                    {\n",
    "                                'selector': 'edge',\n",
    "                                'css': {\n",
    "                                    'content': 'data(edge_label)' #\n",
    "                                }\n",
    "                            }\n",
    "                \n",
    "                ])\n",
    "\n",
    "    for i in range(len(cso.graph.nodes)):\n",
    "        id = int(cso.graph.nodes[i].data['id'])\n",
    "        label = cso.graph.nodes[i].data['node_label']\n",
    "        new_label = f\"{id}: {label}\"\n",
    "        cso.graph.nodes[i].data['node_label'] = new_label\n",
    "\n",
    "\n",
    "    # for i in range(len(cso.graph.edges)):\n",
    "    #     label = cso.graph.edges[i].data['edge_label']\n",
    "    #     new_label = f\"{label}\"\n",
    "    #     cso.graph.edges[i].data['edge_label'] = new_label\n",
    "\n",
    "    return cso\n",
    "    \n",
    "# Test it with output graph\n",
    "import pickle\n",
    "#with open('datasets/DD/data.pkl','rb') as f:\n",
    "with open('../datasets/ZINC_TEST/data.pkl','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "out = vis(data[3])\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SparseGraph:\n",
    "    # Convert a graph to a sparse representation (numpy matrices)\n",
    "    def __init__(self, G):\n",
    "        # Convert a networkx graph (with edge and node labels) to a sparse graph format\n",
    "\n",
    "        # Edge index Matrix\n",
    "        idxs = np.array(G.edges).transpose() # (2,|E|) dim. array idxs[:,j] = [u,v]^T indicates endpoints of j'th edge e=u->v\n",
    "        idxs = np.concatenate((idxs, idxs[[1,0]]), axis=1) # idxs[[1,0]] flips the two rows ie [u,v]^T -> [v,u]^T, so by concat now have (2, 2*|E|)\n",
    "        self.idxs = torch.from_numpy(idxs) #.astype(np.float32))\n",
    "\n",
    "        # Node features\n",
    "        Xv = np.array([G.nodes[idx]['node_label'] for idx in G.nodes]).transpose()#.reshape(-1,1) # Node feature matrix of dim (reshape: (|V|,) -> (|V|,1))\n",
    "        self.Xv = torch.from_numpy(Xv.astype(np.float32))\n",
    "        \n",
    "        # Edges features\n",
    "        Xe = np.array([G.edges[idx]['edge_label'] for idx in G.edges]).transpose()#.reshape(-1,1) # Edge feature matrix of dim (reshape: (|E|,) -> (|E|,1))\n",
    "        Xe = np.concatenate((Xe,Xe), axis=0)\n",
    "        self.Xe = torch.from_numpy(Xe.astype(np.float32))\n",
    "\n",
    "        # Get Graph features\n",
    "        y = G.graph['label']\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "\n",
    "        # Set Batch_idx (just here for compability)\n",
    "        self.batch_idx = torch.zeros((Xv.shape[0]), dtype=torch.int64)\n",
    "\n",
    "    def to_nx(self):\n",
    "        # Convert the sparse graph back to a networkx gaph g\n",
    "\n",
    "        # Convert tensors to numpy\n",
    "        idxs = self.idxs.numpy().astype('int')\n",
    "        Xv = self.Xv.numpy()\n",
    "        Xe = self.Xe.numpy()\n",
    "\n",
    "        g = nx.Graph() # Empty nx graph\n",
    "\n",
    "        # Add edges (nodes added automatically)\n",
    "        for j in range(idxs.shape[1]):\n",
    "            g.add_edge(idxs[0,j], idxs[1,j])\n",
    "        \n",
    "        # Set Node and Edge Weights\n",
    "        nx.set_node_attributes(g, {idx: Xv[idx] for idx in range(Xv.shape[0])}, \"node_label\")\n",
    "        nx.set_edge_attributes(g, {(idxs[0,idx], idxs[1,idx]): Xe[idx] for idx in range(int(Xe.shape[0]/2))}, \"edge_label\")\n",
    "\n",
    "        # TODO: Convert graph label in networkx\n",
    "        return g\n",
    "\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, nx_graph_list):\n",
    "        self.np_sparse_graphs = [SparseGraph(g) for g in nx_graph_list]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.np_sparse_graphs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.np_sparse_graphs[idx]\n",
    "        #return torch.from_numpy(sg.idxs), torch.from_numpy(sg.Xv), torch.from_numpy(sg.Xe), torch.from_numpy(sg.Xe)\n",
    "\n",
    "\n",
    "SG = SparseGraph(data[3])\n",
    "G1 = SG.to_nx()\n",
    "vis(G1)\n",
    "\n",
    "def MyCollate(sparse_graph_list):\n",
    "    #sparse_graph_list = [SparseGraph(data[0]), SparseGraph(data[1]), SparseGraph(data[2]) ]\n",
    "    #sgl = sparse_graph_list\n",
    "\n",
    "    # Create empty SparseGraph Object (avoid calling init, we will initialize here alreadt)\n",
    "    output = SparseGraph.__new__(SparseGraph)\n",
    "\n",
    "    # By joining graphs, the node indexes need to she shifted\n",
    "    # Ie if the first graph has 10 nodes, then for the second graph the node indexes 0,1,2,... --> 10,11,12,...\n",
    "\n",
    "    # compute batch_idx matrix, and a lookup table for how much to shift each graph's nodes indexes by\n",
    "    node_idx_shift = [0] # Lookup table for the node index shift of each graph\n",
    "    batch_idx = []\n",
    "    tot_num_nodes = 0 # Total number of nodes\n",
    "    for i,sg in enumerate(sparse_graph_list):\n",
    "        num_nodes = sg.Xv.shape[0]\n",
    "        tot_num_nodes += num_nodes\n",
    "        node_idx_shift.append(tot_num_nodes)\n",
    "        batch_idx += [i]*num_nodes\n",
    "\n",
    "    # First shift all the node indexes in each graph, and concatenate them\n",
    "    output.idxs = torch.cat([sg.idxs + torch.from_numpy(np.array([node_idx_shift[i], node_idx_shift[i]]).transpose().reshape(-1,1))  # idxs + [idx_shift, idx_shift]^T\n",
    "                            for i, sg in enumerate(sparse_graph_list)],\n",
    "                        dim = 1)\n",
    "\n",
    "    # Change batch_idx type to tensor\n",
    "    output.batch_idx = torch.tensor(np.array(batch_idx), dtype=torch.int64)\n",
    "\n",
    "    # Concatenate Node and Edge feature vectors, and graph labels\n",
    "    output.Xv = torch.cat([sg.Xv  for sg in sparse_graph_list])\n",
    "    output.Xe = torch.cat([sg.Xe for sg in sparse_graph_list])\n",
    "    output.y = torch.cat([sg.y for sg in sparse_graph_list])\n",
    "\n",
    "    return output\n",
    "\n",
    "sgl = [SparseGraph(data[0]), SparseGraph(data[1])] #SparseGraph(data[2]) ]\n",
    "res = MyCollate(sgl)\n",
    "#vis(res.to_nx())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparseGraph(data[0]).batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\holtm\\AppData\\Local\\Temp\\ipykernel_13664\\665360707.py:151: UserWarning: Needed to reshape Xe!!\n",
      "  warnings.warn(\"Needed to reshape Xe!!\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([63, 4])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch_scatter\n",
    "\n",
    "from torch import nn\n",
    "class GNN_U(torch.nn.Module):\n",
    "    # TODO: Actually implement this! Just dummy so far (!!depth attribute!!)\n",
    "\n",
    "    def __init__(self, in_features, out_features, depth):\n",
    "        super(GNN_U, self).__init__()\n",
    "        self.fc = nn.Linear( in_features, out_features)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x) \n",
    "        return x\n",
    "\n",
    "class GNN_M(torch.nn.Module):\n",
    "    # TODO: Actually implement this! Just dummy so far (!!depth attribute!!)\n",
    "    def __init__(self, in_features, out_features, depth):\n",
    "        super(GNN_M, self).__init__()\n",
    "        self.fc = nn.Linear( in_features, out_features)\n",
    "        self.relu = torch.nn.ReLU() \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x) \n",
    "        return x\n",
    "\n",
    "\n",
    "class GNN_layer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, Xe_width, scatter_func='SUM', U_depth=2, M_depth=2, M_width=2):\n",
    "        super(GNN_layer, self).__init__()\n",
    "\n",
    "        # Initialize Scatter function\n",
    "        if type(scatter_func) == type('str'):\n",
    "            import torch_scatter\n",
    "            if scatter_func.lower()=='sum':\n",
    "                self.scatter_agg = torch_scatter.scatter_sum\n",
    "            elif scatter_func.lower()=='max':\n",
    "                self.scatter_agg = torch_scatter.scatter_max\n",
    "            elif scatter_func.lower()=='mean':\n",
    "                self.scatter_agg = torch_scatter.scatter_mean\n",
    "            else:\n",
    "                import warnings\n",
    "                warnings.warn(\"scatter_function unknown! Defaulting to \\\"SUM\\\"\")\n",
    "                self.scatter_agg = torch_scatter.scatter_add\n",
    "        else: \n",
    "            # Custom scatter function\n",
    "            self.scatter_agg = scatter_func\n",
    "\n",
    "        # Initialize M and U Neural Nets\n",
    "        self.M = GNN_M(in_features + Xe_width, M_width, M_depth)\n",
    "        self.U = GNN_U(in_features + M_width, out_features, U_depth)\n",
    "\n",
    "        # Define parameter list (needed for optimizer)\n",
    "        self.param_list = list(self.M.parameters()) + list( self.U.parameters())\n",
    "\n",
    "    def forward(self, H, sparse_graph):\n",
    "        Y = self.M.forward(torch.cat((H[sparse_graph.idxs[0,:]], sparse_graph.Xe), dim=1)) # (2|E|, in_features + Xe_width) -> (2|E|, M_width)\n",
    "        # TODO: Special case for max\n",
    "        Z = self.scatter_agg(Y, sparse_graph.idxs[1,:], dim=0) # (2|E|, M_width) -> (|V|, M_width)\n",
    "        return self.U.forward(torch.cat((H,Z), dim=1)) # (|V|, H_width + M_width) -> (|V|, out_features)\n",
    "\n",
    "\n",
    "class GNN_pool(torch.nn.Module):\n",
    "    def __init__(self, scatter_func='sum'):\n",
    "        super(GNN_pool, self).__init__()\n",
    "        \n",
    "        # Initialize Scatter function\n",
    "        if type(scatter_func) == type('str'):\n",
    "            import torch_scatter\n",
    "            if scatter_func.lower()=='sum':\n",
    "                self.scatter_agg = torch_scatter.scatter_sum\n",
    "            elif scatter_func.lower()=='max':\n",
    "                self.scatter_agg = torch_scatter.scatter_max\n",
    "            elif scatter_func.lower()=='mean':\n",
    "                self.scatter_agg = torch_scatter.scatter_mean\n",
    "            else:\n",
    "                import warnings\n",
    "                warnings.warn(\"scatter_function unknown! Defaulting to \\\"SUM\\\"\")\n",
    "                self.scatter_agg = torch_scatter.scatter_add\n",
    "        else: \n",
    "            # Custom scatter function\n",
    "            self.scatter_agg = scatter_func\n",
    "\n",
    "        # Parameter list (empty)\n",
    "        self.param_list = []\n",
    "\n",
    "    def forward(self, H, sparse_graph):\n",
    "        return torch_scatter.scatter_sum(H, sparse_graph.batch_idx, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GNN_virtual_node(torch.nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(GNN_virtual_node, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, in_features)\n",
    "        self.relu = torch.nn.ReLU() \n",
    "        self.param_list = self.parameters()\n",
    "\n",
    "    def forward(self, H, sparse_graph):\n",
    "        # Compute sum over all nodes for each graph, H_sum_graph[i] = sum of H over all nodes in the i'th subgrpah\n",
    "        H_sum_graph = torch_scatter.scatter_sum(H, sparse_graph.batch_idx, dim=0)\n",
    "        H_sum_graph = self.fc.forward(H_sum_graph)\n",
    "        H_sum_graph = self.relu.forward(H_sum_graph)\n",
    "\n",
    "        # Cast graph sum back to every node, and then add to H\n",
    "        return H + H_sum_graph[sparse_graph.batch_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, Xv_width, Xe_width):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.layers = []\n",
    "        \n",
    "        # First layer, (input dimension must match the initial dimension)\n",
    "        self.layers.append(GNN_layer(in_features=Xv_width, out_features=2, Xe_width=Xe_width, scatter_func='sum', U_depth=2, M_depth=2, M_width=2))\n",
    "\n",
    "        # TODO: Add more layers (and actually figure out what needs to be done here)\n",
    "        self.layers.append(GNN_virtual_node(2))\n",
    "\n",
    "        self.layers.append(GNN_layer(in_features=2, out_features=4, Xe_width=Xe_width, scatter_func='sum', U_depth=2, M_depth=2, M_width=2))\n",
    "\n",
    "\n",
    "        # Final layer (For node level classication, should be exactly the size of the node output, for anything else, it should be different)\n",
    "        #self.layers.append(GNN_pool('max'))\n",
    "\n",
    "        # Build list of parameters (needed for optimizer)\n",
    "        self.param_list = []\n",
    "        for layer in self.layers:\n",
    "            self.param_list += layer.param_list\n",
    "\n",
    "\n",
    "    def forward(self, sparse_graph):\n",
    "        # Initial Hidden node layers\n",
    "        H = sparse_graph.Xv\n",
    "\n",
    "        # Reshape if neccessary\n",
    "        if len(H.shape)==1:\n",
    "            H = H.reshape((-1,1))\n",
    "\n",
    "        # Reshape Edge feature matrix if neccessarry\n",
    "        if len(sparse_graph.Xe.shape)==1:\n",
    "            sparse_graph.Xe = sparse_graph.Xe.reshape((-1,1))\n",
    "            import warnings\n",
    "            warnings.warn(\"Needed to reshape Xe!!\")\n",
    "\n",
    "        # Pass \n",
    "        for layer in self.layers:\n",
    "            H = layer.forward(H, sparse_graph)\n",
    "        return H        \n",
    "\n",
    "\n",
    "#Simple Example\n",
    "gnn = GNN(Xv_width=1, Xe_width=1)\n",
    "#sparse_graph = SparseGraph(data[0])\n",
    "sparse_graph = MyCollate([SparseGraph(data[0]), SparseGraph(data[1]), SparseGraph(data[2])])\n",
    "H = gnn.forward(sparse_graph)\n",
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.4589,  0.1252, 10.1779,  3.5807],\n",
       "        [ 9.2291,  0.0612, 14.4674,  5.2726],\n",
       "        [ 9.6784,  0.0934, 15.1854,  5.6036]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh = torch_scatter.scatter_sum(H, batch_idx, dim=0)\n",
    "fc = nn.Linear(4,4)\n",
    "fc.forward(hh) +hh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(data)\n",
    "gnn = GNN(1,1)\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(gnn.param_list, lr=0.005)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=MyDataset(data), batch_size=10, collate_fn=MyCollate)\n",
    "\n",
    "loss_fn = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (A, H, labels) in enumerate(train_loader):\n",
    "    # Sets the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # do forward pass\n",
    "    output = model((A, H))\n",
    "    # calculate loss\n",
    "    loss = loss_fn(output, labels)\n",
    "    # backpropagate loss\n",
    "    loss.backward()\n",
    "    # do parameter update\n",
    "    optimizer.step()\n",
    "    # calculate labels from predictions\n",
    "    scores, predictions = torch.max(output.data, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('gll')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3325763147d7b6a719f3ec3969238f3ee9907e3239cd74fe71b5fc36bf844357"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
